{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning 101: Probabilistic Image Classification with Gaussian Naive Bayes\n",
    "\n",
    "#### Author: Alex - Super Make Something\n",
    "#### Date: 2020-06-05\n",
    "\n",
    "#### Supplemental YouTube video: Machine Learning 101: Image classification with Naive Bayes (NVIDIA Jetson Xavier NX!)\n",
    "\n",
    "#### Video URL: https://youtu.be/UkXMP89B2pM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most common uses of machine learning is image classification, where a computer determines what it is \"seeing\" in an image based on various items it has seen before.  But how exactly does a machine learning algorithm go from raw pixel values to determining whether the object in an image is a dog, a cat, or a banana?  One way to do it is to use probability and a classifier known as \"Gaussian Naive Bayes\" (GNB).\n",
    "\n",
    "The code below sets up, trains, and tests a GNB classifier to tell the difference between various types of fruit.  This example uses the \"Fruits-360\" dataset, a collection of over 90,000 100x100 pixel images of different types of fruits, vegetables, and nuts.  Images in this set are divided into separate training and testing folders, each of which contain separate additional folders for different food types.  The 761MB \"Fruits-360\" zip file can be downloaded [here](https://github.com/Horea94/Fruit-Images-Dataset).  This file should be unzipped into the same directory as this Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Background: The image classification pipeline\n",
    "\n",
    "<img src=\"img/classificationPipeline.png\" width=\"75%\">\n",
    "\n",
    "<p style=\"text-align:center\"><i>Illustration of the image classification pipeline.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While many different types of machine learning algorithms exist to classify images, all of them follow the same basic steps.  Most two dimensional digital images are made up of an array of red, green, and blue pixel values, which must first be loaded by the program that implements the image classification algorithm.  \n",
    "\n",
    "Once loaded, these arrays are then modified in the **pre-processing step** to adjust the image contrast and brigthness, to transform the pixel values into a different color space, or to crop and resize images so that all input images have the same size and resolution.  \n",
    "\n",
    "After this, the **feature extraction step** extracts information or \"features\" from these images, which a machine learning algorithm uses to understand the content of the image.  These features can either be predetermined by the user (i.e. what is the average color of the image and how many pixels in the image are white?) or be automatically generated based on the structure of the image classification algorithm (as is the case with image classification approaches that use Artificial Neural Networks).  (The algorithm implemented in this Notebook, \"Gaussian Naive Bayes,\" uses predetermined features that are specified by the user for image classification.  Feel free to experiment by implementing new features to see how it affects classification accuracy!)  \n",
    "\n",
    "After all of the required features are extracted from an image, the **classification step** compares these to a library of feature values for various different objects that the image classifier has seen before during training.  It then decides what is in the image by determining the closest match between the set of pre-trained feature values and the features extracted from the input image.  \n",
    "\n",
    "To understand what decisions our image classifier made and to test how well the image classifier works, we will pass a set of images that we know the content of into the classifier, and compare what the it thinks it sees to what is actually in the image.  This will be done by feeding images from various folders in the Fruits-360 dataset into the trainied classifier and seeing if the predicted fruit matches the folder the image came from.\n",
    "\n",
    "*One quick bit of terminology:* Each different object type a classifier can recognize is called a <b>class</b>.  As its name suggests, a <b>class</b>ifier tries to determine what class an input image belongs to, based on all of the different classes that it encountered during the algorithm's training stage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Gaussian Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes classifiers are a family of classification algorithms that use probability to determine what class an object belongs to.  Like their name suggests, these probabilistic classifiers rely on Bayes theorem, which is written as: \n",
    "\n",
    "\\begin{equation}\n",
    "P(Y|\\textbf{X})\\ \\ \\ = \\ \\ \\ \\frac{P(Y) P(\\textbf{X}|Y)}{P(\\textbf{X})},\n",
    "\\end{equation}\n",
    "\n",
    "where $P(Y|\\textbf{X})$ is the probability that a sample belongs to class $Y$ given that it has the feature values $\\textbf{X}$, $P(Y)$ is the probability of randomly drawing a sample that belongs to class $Y$ out of the dataset, $P(\\textbf{X}|Y)$ is the probability that a sample with feature values $X$ belongs to class $Y$, and $P(X)$ is the probability that $X$ occurs.  This value is calculated using the extracted feature values for a sample we are interested in classifying for class we want to differentiate between.  Classification is then accomplished by simply picking the class that has the highest probability.\n",
    "\n",
    "When classifying samples into a known number of classes, $P(X)$ is a constant for all classes, so it does not need to be calculated, which makes the math easier and the classifier a bit faster.  Instead, it is possible to write \n",
    "\n",
    "\\begin{equation}\n",
    "P(Y|\\textbf{X}) \\ \\ \\ \\alpha \\ \\ \\ P(Y) P(\\textbf{X}|Y),\n",
    "\\end{equation}\n",
    "\n",
    "where $\\alpha$ denotes that the two quantities are proportional to each other.  This means that the final calculation will not give the actual probability that this event will occur in the real world, but since we only care about the relative likelihood that a sample belongs to one of the classes we are interested in, this is okay.\n",
    "\n",
    "<img src=\"img/multiFeatureClassification.png\" width=\"75%\">\n",
    "\n",
    "<p style=\"text-align:center\"><i>The advantage of multi-feature classification.</i>\n",
    "\n",
    "Based on what the classifier is trying to classify, it is unlikely that a single feature will be able to perfectly categorize every sample the classifier will see.  Therefore, it is necessary to use multiple features to describe the objects in each class.  In the real-world, however, it is likely that several features depend on each other -- for example how sweet a person thinks a food is depends on the amount of sugar in the food.  This quantity is known as the joint probability, but can be incredibly difficult to calculate for all of a classifier's potential features.  \n",
    "    \n",
    "This is where the \"naive\" portion of Naive Bayes comes in -- instead of calculating the joint probability, a Naive Bayes classifier simply treats all of the features as independent, meaning that the conditional probability $P(\\textbf{X}|Y=y_i)$ of a new sample with feature vector $X$ belonging to the class $y_i$ can be calculated for each feature separately, and then simply multiplied together:\n",
    "    \n",
    "\\begin{equation}\n",
    "P(Y=y_i|\\textbf{X}) \\ \\ \\ \\alpha \\ \\ \\ P(Y=y_i) \\prod_{i=1}^{n}P(\\textbf{X}|Y=y_i).\n",
    "\\end{equation}\n",
    "    \n",
    "The last remaining piece is to calculate the conditional probability for each feature.  One way to do this is to assume that the conditional probability for each feature follows a Gaussian distribution, where the likelihood can be described by a mean and standard deviation:\n",
    "\n",
    "\\begin{equation}\n",
    "P(X|Y=y_i) =  \\frac{1}{\\sigma_i \\sqrt{2\\pi}} e^{-(x-\\mu_i)^2/(2\\sigma_i^2)}\n",
    "\\end{equation}\n",
    "    \n",
    "While this equation looks complicated, all that is needed to calculate the probability that a new sample belongs to a certain class is the values of that new sample's features $x$, and the mean values $\\mu_i$ and standard deviations $\\sigma_i$ of those features for all of the classes that you are trying to differentiate between, which is calculated from your training data.  This makes Gaussian Naive Bayes a fast and easy image classification scheme that is useful for many different scenarios.  \n",
    "    \n",
    "Now that we know what a Gaussian Naive Bayes classifier is, let's implement it in Python and use it to classify images!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required libraries\n",
    "\n",
    "This classifier is built using `Matplotlib` to load and plot images, `Numpy` to manipulate image and feature arrays, and `os` to generate and create file paths and folders.  To analyze the results, the code also uses the data manipulation library `Pandas` and the data visualization library `Seaborn` to generate a confusion matrix that makes it easy to see where the classifier worked well and where it was confused.  Finally, the file operations library `shutil` is used to delete any existing results from an old run to keep everything organized and the time library is used for profiling purposes to get an estimate of how fast portions of the code execute.\n",
    "\n",
    "The cell below first imports all of the required libraries.\n",
    "\n",
    "Note: You can run individual Jupyter Notebook cells with SHIFT+ENTER.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import matlib\n",
    "import os\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define required supporting methods\n",
    "\n",
    "The cell below defines the <i>trainFeature_statsColors()</i> method, which loads all images in a specified training directory and calculates the mean and standard deviation of all pixels in the images' red, green, and blue color channels after tresholding out the white background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainFeature_statsColors(trainDir):  # Calculate feature -- mean and standard deviation of RGB values\n",
    "\n",
    "    filenames = os.listdir(trainDir)  # Get filenames in \"trainDir\" directory\n",
    "    redVals = greenVals = blueVals = np.zeros([1, 0])  # Declare empty numpy arrays to save all red, green, and blue channel values\n",
    "\n",
    "    for file in filenames[:-1]:  # Iterate through each picture in the specified training directory\n",
    "        baseImage = plt.imread(trainDir + file)\n",
    "        image = baseImage.copy()  # Create copy of the image -- workaround for write permission modification error in some versions of numpy\n",
    "        image = np.reshape(image, (-1, 3))  # Reshape image into a 10,000x3 array where the first, second, and third column correspond to the red, green, and blue color value of each pixel, respectively\n",
    "        image = image[~np.all(image > 250, axis=1)]  # Remove all white background pixels in the image\n",
    "        redVals = np.append(redVals, image[:, 0])  # Append all red channel values of current image to redVals array\n",
    "        greenVals = np.append(greenVals, image[:, 1])  # Append all green channel values of current image to greenVals array\n",
    "        blueVals = np.append(blueVals, image[:, 2])  # Append all blue channel values of current image to blueVals array\n",
    "\n",
    "    avgR = np.mean(redVals)  # Calculate the mean value of all red values\n",
    "    stdR = np.std(redVals)  # Calculate the standard deviation of all red values\n",
    "    avgG = np.mean(greenVals)  # Mean of all green values\n",
    "    stdG = np.std(greenVals)  # Standard deviation of all all green values\n",
    "    avgB = np.mean(blueVals)  # Mean of all blue values\n",
    "    stdB = np.std(blueVals)  # Standard deviation of all blue values\n",
    "    \n",
    "    return avgR, stdR, avgG, stdG, avgB, stdB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below defines the <i>trainFeature_statsWPixels()</i> method, which loads each image in a specified training directory and calculates the mean and standard deviation of the number of white background pixels in each image.  Due to the way that the dataset was generated, this value can be thought of as relating to each fruit's size and shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainFeature_statsWPixels(trainDir):  # Calculate feature -- mean and standard deviation of white background pixels\n",
    "\n",
    "    filenames = os.listdir(trainDir)\n",
    "    noWhitePix = np.zeros([1,0])  # Declare empty numpy array that stores the number of white background pixels in each image\n",
    "\n",
    "    for file in filenames[:-1]:\n",
    "        baseImage = plt.imread(trainDir + file)\n",
    "        image = baseImage.copy()\n",
    "        image = np.reshape(image, (-1, 3))\n",
    "        image = image[~np.all(image < 250, axis=1)]  # Remove all non-white pixels in each image\n",
    "        noWhitePix = np.append(noWhitePix, image.shape[0])  # Append number of white pixels in current image to noWhitePix array\n",
    "\n",
    "    avgWPix = np.mean(noWhitePix)  # Calculate the mean value of number of white pixels\n",
    "    stdWPix = np.std(noWhitePix)  # Calculate the standard deviation of number of white pixels\n",
    "    return avgWPix, stdWPix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell calculates the prior probability for each class, i.e. the probability that a sample from each class can be drawn at random.  This value is calculated simply as the number of images in each class divided by the total number of images used during training.  Calculating actual prior probabilities is hard (if not impossible for real-world image classification applications), so this calculation makes the assumption that the number of examples in each class in the training dataset is close to the number of items in each class in the testing dataset (or the classifier's chance of encountering these items at random in the real world.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcPriors(classes, trainDir):  # Calculate prior probabilities for each class\n",
    "    \n",
    "    priorProb = np.empty([1,0])\n",
    "    \n",
    "    for item in classes:\n",
    "        filenames = os.listdir(trainDir + item + '/')  # Iterate through every folder in the 'classes' list\n",
    "        priorProb = np.append(priorProb,len(filenames))  # Add total number of images in that folder to an array\n",
    "    \n",
    "    priorProb = priorProb/np.sum(priorProb)  # Divide the number of images in a folder by the total number of images\n",
    "    return priorProb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two cells define the <i>calcFeature_statsColor()</i> and <i>calcFeature_statsWPixels()</i> methods.  These code blocks essentially do the same thing as the <i>trainFeature_statsColor()</i> and <i>trainFeature_statsWPixels()</i> methods, but for one image, whose path is defined by the _testImgPath_ variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcFeature_statsColors(testImgPath):  # Calculate feature -- mean RGB values\n",
    "\n",
    "    baseImage = plt.imread(testImgPath)\n",
    "    image = baseImage.copy()\n",
    "    image = np.reshape(image, (-1, 3))\n",
    "    image = image[~np.all(image > 250, axis=1)]\n",
    "    \n",
    "    avgR = np.mean(image[:,0])  # Calculate the mean of all values in the red color channel\n",
    "    avgG = np.mean(image[:,1])  # Calculate the mean of all values in the green color channel\n",
    "    avgB = np.mean(image[:,2])  # Calculate the mean of all values in the blue color channel\n",
    "    return avgR, avgG, avgB    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcFeature_statsWPixels(testImgPath):   # Calcuate feature -- number of white pixels in the image\n",
    "\n",
    "    baseImage = plt.imread(testImgPath)\n",
    "    image = baseImage.copy()\n",
    "    image = np.reshape(image, (-1, 3))\n",
    "    image = image[~np.all(image < 250, axis=1)]\n",
    "    noWhitePx = image.shape[0]\n",
    "    return noWhitePx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below defines the <i>calcLikelihood()</i> method, which is at the heart of the GNB classification algorithm.  Using the features calculated for a given inmage, in addition to the mean and standard deviation feature arrays calculated during training, this method returns an array of conditional probabilities that describe how likely it is that an image with the given set of feature values belongs to a certain class.  Later, this array is multiplied with the prior array that was also calculated during training, in order to calculate the probability that the image belongs to each class given its feature values.  To speed up classification, the input feature array is appropriately sized to match the dimensions of the mean and standard deviation arrays so that all likelihood calculations can be done without a for loop using matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcLikelihood(featureVec, meanFeatureVals, stdFeatureVals):\n",
    "    \n",
    "    featureLikelihood = 1/np.sqrt(2 * np.pi * stdFeatureVals ** 2) * np.exp(-1 * (featureVec - meanFeatureVals) ** 2 / (2 * stdFeatureVals ** 2))\n",
    "    likelihoods = np.prod(featureLikelihood, axis=1)\n",
    "    return likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the classifier\n",
    "\n",
    "Let's first define a list of fruits we are interested in classifying and tell the algorithm where the training data is located.  Different fruits can be selected by modifying the entries in the 'classes' list.  These entries correspond to folder names within the training and testing directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['Apple Red 1', 'Banana', 'Blueberry', 'Lemon', 'Limes']  # Fruits to classify \n",
    "trainDir = \"Fruit-Images-Dataset-master/Fruit-Images-Dataset-master/Training/\"  # Directory of training images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to train the classifier!  The code below calculates the mean and standard deviation of features using the training methods defined above.  After training, the array of feature values and prior probabilities are saved to file so that classification can occur later without needing to re-run training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating feature values for Apple Red 1: Done\n",
      "Calculating feature values for Banana: Done\n",
      "Calculating feature values for Blueberry: Done\n",
      "Calculating feature values for Lemon: Done\n",
      "Calculating feature values for Limes: Done\n",
      "Calculating priors: Done\n",
      "Saving Array of Training Values: Done\n",
      "Training complete!  Elapsed Time: 29.747912645339966 seconds.\n"
     ]
    }
   ],
   "source": [
    "timeStart = time.time()  # Get current time\n",
    "\n",
    "# Declare empty array to store feature values\n",
    "noFeatures = 4  # Number of features\n",
    "meanFeatureVals = np.empty((0, noFeatures))\n",
    "stdFeatureVals = np.empty((0, noFeatures))\n",
    "\n",
    "for fruit in classes:  # Calculate feature values for all classes listed in the 'classes' list\n",
    "    print('Calculating feature values for ' + fruit + ':', end=' ')\n",
    "    avgR, stdR, avgG, stdG, avgB, stdB = trainFeature_statsColors(trainDir + fruit + '/')  # Calculate R,G,B color features\n",
    "    avgWPix, stdWPix = trainFeature_statsWPixels(trainDir + fruit + '/')  # Calculate number of white pixels feature\n",
    "    meanFeatureVals = np.vstack((meanFeatureVals, np.array([avgR, avgG, avgB, avgWPix])))  # Save mean values into array\n",
    "    stdFeatureVals = np.vstack((stdFeatureVals, np.array([stdR, stdG, stdB, stdWPix])))  # Save std values into array\n",
    "    print('Done')\n",
    "\n",
    "print('Calculating priors:', end=' ')\n",
    "priorProbabilities = calcPriors(classes, trainDir)  # Calculate the prior probability for each class\n",
    "print('Done')\n",
    "\n",
    "print(\"Saving Array of Training Values:\", end=' ')\n",
    "np.savez('gnbClassifier',meanFeatureVals, stdFeatureVals, priorProbabilities)  # Save feature value and prior arrays so that they can be loaded later\n",
    "print('Done')\n",
    "\n",
    "print('Training complete!  Elapsed Time: ' + str(time.time() - timeStart) + ' seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifying images\n",
    "\n",
    "Time to classify some images!  Let's begin by selecting our testing data.  The code below first re-loads the feature and prior probability arrays.  Next, it chooses ten random images from each of the trained on classes using images in the test dataset, and saves the filepath to each image into a list.  (Choosing ten items from each class violates the previous assumption that was used to calculate prior probabilities, but the number of images in each training folder is roughly equal, so this is okay.)  For development purposes, the <i>random()</i> method is seeded with a value so that the same ten images are chosen at \"random\" whenever the program is run.  Comment this line out to grab truly random images from each folder.  \n",
    "\n",
    "During this process, the code also generates an array of the actual labels for each image based on which folder the image came from, which we will use later to examine how accurately the classifier identified each fruit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('gnbClassifier.npz')  # Load the .npz file containing feature value and prior arrays\n",
    "meanFeatureVals = data['arr_0']  # Assign each array to a variablle\n",
    "stdFeatureVals = data['arr_1']\n",
    "priorProb = data['arr_2']\n",
    "\n",
    "testDir = \"Fruit-Images-Dataset-master/Fruit-Images-Dataset-master/Test/\"  # Directory of testing images\n",
    "np.random.seed(1)  # Seed the random number generator to generate the same \"random\" numbers each time\n",
    "images = []  # Declare empty list to store filepaths to testing images\n",
    "trueLabels = []  # Declare empty list to store true labels\n",
    "\n",
    "for fruit in classes:\n",
    "    filenames = os.listdir(testDir + fruit + '/')  # Get list of all filenames in a testing\n",
    "    for i in range(10):  # Grab ten random images\n",
    "        randomIndex = np.random.randint(low=0, high=len(filenames))  # Generate a random integer between [0, number of images in the folder]\n",
    "        images.append(testDir + fruit + '/' + filenames[randomIndex])  # Add path to random image to list\n",
    "        trueLabels.append(fruit)  # Add true label to list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, each image is loaded and the color and white pixel features are extracted from each image using the <i>calcFeature_statsColors()</i> and <i>calcFeature_statsWPixels()</i> methods that are defined above.  After this the likelihood that the image came from each class is calculated using the <i>calcLikelihood()</i> method, which is then multiplied with the array of prior values.  This calculation results in a set of numbers proportional to the probability that the image belongs to each of the classes given its feature values.  The number with the highest probability is the class chosen by the classifier, which is then written to the list 'chosenLabels'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification complete!  Elapsed Time: 0.05695295333862305 seconds.\n"
     ]
    }
   ],
   "source": [
    "timeStart = time.time()  # Get current time\n",
    "\n",
    "predLabels = []  # Declare empty list to store labels predicted by the classifier\n",
    "\n",
    "for item in images:  # For all images in the test set\n",
    "    avgR, avgG, avgB  = calcFeature_statsColors(item)  # Extract R, G, B color features\n",
    "    noWhitePix = calcFeature_statsWPixels(item)  # Extract number of white pixels feature\n",
    "    featureVec = np.array([avgR, avgG, avgB, noWhitePix])  # Place all features into an array\n",
    "    featureVec = np.matlib.repmat(featureVec, len(classes), 1)  # Replicate the array to match the feature array size for matrix multiplication\n",
    "    likelihoods = calcLikelihood(featureVec, meanFeatureVals, stdFeatureVals)  # Calculate likelihoods\n",
    "    posteriorProb = likelihoods*priorProb  # Multiply the likelihoods with the prior probabilities\n",
    "    predClass = np.argmax(posteriorProb)  # Find the index with the largest probability\n",
    "    predLabels.append(classes[predClass])  # Add predicted label to list\n",
    "\n",
    "print('Classification complete!  Elapsed Time: ' + str(time.time() - timeStart) + ' seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating classifier performance\n",
    "\n",
    "Time to find out how well the classifier worked!  To do this, we will generate a confusion matrix, a table of values that makes it easily to see items were classified correctly and which items the classifier confused.   To look at individual results in more detail, the classifier also writes each test image overlaid with the chosen label to a folder called 'Results.'  For a full explanation of the results generated with this code, check out the corresponding episode on YouTube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual classification results saved to the 'Results/' directory.\n"
     ]
    }
   ],
   "source": [
    "confusionMatrix = pd.crosstab(pd.Series(trueLabels), pd.Series(predLabels))  # Cross-tabulate actual and predicted labels\n",
    "sn.heatmap(confusionMatrix, cbar=False, annot=True, cmap='Blues', square=True)  # Create heatmap\n",
    "plt.xlabel(\"Predicted Fruit\")  # Add labels\n",
    "plt.ylabel(\"Actual Fruit\")\n",
    "plt.show()  # Show the plot\n",
    "\n",
    "resultsFolder = 'Results/'  # Specify results folder\n",
    "resFilenames = [sub.replace(testDir, resultsFolder) for sub in images]  # Generate filenames for saving annotated results\n",
    "\n",
    "if os.path.exists(resultsFolder):  # Remove any existing results\n",
    "        shutil.rmtree(resultsFolder)\n",
    "\n",
    "for fruit in classes:  # Create directories in which to save results\n",
    "    if not os.path.exists(resultsFolder + fruit + '/'): #check for existence of results directory\n",
    "        os.makedirs(resultsFolder + fruit + '/') #create it if it doesn't exist\n",
    "\n",
    "for index, item in enumerate(images):  # Iterate through all test images\n",
    "    baseImage = plt.imread(item)\n",
    "    plt.imshow(baseImage)\n",
    "    plt.axis('off')\n",
    "    if trueLabels[index] == predLabels[index]:  # If the actual and predicted label match\n",
    "        plt.text(0, 0, predLabels[index], bbox=dict(facecolor='green', alpha=0.5))  # Overlay green box with predicted label\n",
    "    else:  # If the actual and predict label do not match\n",
    "        plt.text(0, 0, predLabels[index], bbox=dict(facecolor='red', alpha=0.5))  # Overlay red box with predicted label\n",
    "    plt.savefig(resFilenames[index])  # Save the annotated image\n",
    "    plt.close()\n",
    "    \n",
    "print('Individual classification results saved to the \\'' + resultsFolder + '\\' directory.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Gaussian Naive Bayes classifiers are an intuitive, simple, and computationally inexpensive way to classify images.  While this example focused on classifying whole images that only contained a single object, this approach could also be used on a subset of pixels in images to identify multiple objects or to accomplish image segmentation, or on an entirely different data format like sound files or text.  Feel free to use this code as a jumping off point for implementing your own GNB classifier and experiment with different classes and features!\n",
    "\n",
    "If you liked this tutorial and want to see other cool projects, please consider subscribing to [Super Make Something channel on YouTube](https://www.youtube.com/supermakesomething)!  Also feel free to get in touch about other Machine Learning topics you would like to see covered on the channel!  Thanks for checking this tutorial out and be sure to [like the video that accompanies this code](https://youtu.be/UkXMP89B2pM) -- now go Super Make Something!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
